# -*- coding: utf-8 -*-
"""suc_rust_ollama_chat_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SsbBNk6mr2sZXWUoMKhjXzLnwg1AhL0c
"""







curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &
!ollama pull llama3.2

"""https://github.com/Juan-LukeKlopper/OxideAgent

# Task
Install and run Rust in Google Colab.

## Install rust

### Subtask:
Install the Rust toolchain using rustup.

**Reasoning**:
Download and execute the rustup installation script.
"""

!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y

"""**Reasoning**:
Verify the installation by checking the rustc version and add rust to the PATH.


"""

!source "$HOME/.cargo/env" && rustc --version

"""## Verify installation

### Subtask:
Check the Rust version to ensure the installation was successful.

**Reasoning**:
Source the Cargo environment file and check the Rust compiler version.
"""

!source "$HOME/.cargo/env" && rustc --version

"""## Write rust code

### Subtask:
Create a simple Rust program.

**Reasoning**:
Create a file named main.rs and write a simple Rust program that prints "Hello, Rust!".
"""

with open("main.rs", "w") as f:
  f.write('fn main() {\n    println!("Hello, Rust!");\n}')

"""## Compile and run rust code

### Subtask:
Compile and run the Rust program.

**Reasoning**:
Compile and run the Rust program using the Rust compiler and execute the compiled binary.
"""

!source "$HOME/.cargo/env" && rustc main.rs && ./main

"""## Summary:

### Data Analysis Key Findings

*   The Rust toolchain, including `rustc` and `cargo`, was successfully installed in the Google Colab environment using `rustup`.
*   The installed Rust version is 1.89.0.
*   A simple Rust program named `main.rs` was created.
*   The Rust program was successfully compiled and executed, producing the output "Hello, Rust!".

### Insights or Next Steps

*   The successful installation and execution of a basic Rust program demonstrate the feasibility of using Rust within Google Colab for various tasks.
*   Further steps could involve exploring the use of Cargo for managing Rust projects and dependencies, or integrating Rust code with Python for performance-critical operations.

"""









!ollama list















"""############################################

### aشغال
"""

!ollama list

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!source "$HOME/.cargo/env" && cargo new rust_llama_chat

# Commented out IPython magic to ensure Python compatibility.
# %cd rust_llama_chat

!source "$HOME/.cargo/env" && cargo add reqwest --features json
!source "$HOME/.cargo/env" && cargo add tokio --features full

!source "$HOME/.cargo/env" && cargo run

!source "$HOME/.cargo/env" && cargo clean
!source "$HOME/.cargo/env" && cargo build

/content/rust_llama_chat/Cargo.toml
[package]
name = "rust_llama_chat"
version = "0.1.0"
edition = "2024"

[dependencies]
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }

/content/rust_llama_chat/src/main.rs

use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Serialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Deserialize)]
struct OllamaResponse {
    response: String,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let client = Client::new();
    let model = "llama3.2:latest";
    let url = "http://localhost:11434/api/generate";

    println!("🦀 مرحبًا بك في شات Rust مع Llama 3.2!");
    loop {
        print!("أنت: ");
        io::stdout().flush()?;
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;

        let input = input.trim();
        if input == "exit" {
            println!("👋 تم إغلاق الشات.");
            break;
        }

        let request_body = OllamaRequest {
            model: model.to_string(),
            prompt: input.to_string(),
            stream: false,
        };

        let response = client
            .post(url)
            .json(&request_body)
            .send()
            .await?
            .json::<OllamaResponse>()
            .await?;

        println!("🤖 Llama: {}", response.response);
    }

    Ok(())
}

!source "$HOME/.cargo/env" && cargo run

"""### شغال

##################################
"""







"""### يمكنك بناء وكيل بحث عميق (Deep Web Search Agent) بلغة Rust يقوم بالآتي:"""



/content/rust_llama_chat/src/main.rs

use reqwest::Client;
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use std::error::Error;

#[derive(Serialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Deserialize)]
struct OllamaResponse {
    response: String,
}

async fn search_web(query: &str) -> Result<Vec<String>, Box<dyn Error>> {
    let client = Client::new();
    let search_url = format!("https://html.duckduckgo.com/html/?q={}", urlencoding::encode(query));
    let res = client.get(&search_url).send().await?.text().await?;
    let document = Html::parse_document(&res);
    let selector = Selector::parse(".result__snippet").unwrap();

    let snippets: Vec<String> = document
        .select(&selector)
        .take(3)
        .map(|el| el.text().collect::<String>())
        .collect();

    Ok(snippets)
}

async fn summarize_with_ollama(context: &str, question: &str) -> Result<String, Box<dyn Error>> {
    let client = Client::new();
    let prompt = format!("Based on the following search results:\n{}\nAnswer this question: {}", context, question);

    let request_body = OllamaRequest {
        model: "llama3.2:latest".to_string(),
        prompt,
        stream: false,
    };

    let res = client
        .post("http://localhost:11434/api/generate")
        .json(&request_body)
        .send()
        .await?
        .json::<OllamaResponse>()
        .await?;

    Ok(res.response)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    println!("🔍 أدخل سؤالك للبحث:");
    let mut input = String::new();
    std::io::stdin().read_line(&mut input)?;
    let question = input.trim();

    println!("⏳ يتم البحث...");
    let snippets = search_web(question).await?;
    let context = snippets.join("\n");

    println!("⏳ يتم تلخيص النتائج...");
    let answer = summarize_with_ollama(&context, question).await?;

    println!("🤖 النتيجة:\n{}", answer);
    Ok(())
}

/content/rust_llama_chat/Cargo.toml


[package]
name = "rust_llama_chat"
version = "0.1.0"
edition = "2024"

[dependencies]
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
scraper = "0.17"
url = "2.5"
urlencoding = "2.1.3"

!source "$HOME/.cargo/env" && cargo clean
!source "$HOME/.cargo/env" && cargo build

!source "$HOME/.cargo/env" && cargo add urlencoding

!source "$HOME/.cargo/env" && cargo run

"""https://www.kimi.com/chat/d2v2un13om1ns60dqo8g

ممتاز! النموذج استطاع:

- استخراج ثلاث نقاط واضحة من النتائج التي جمعها الوكيل  
- ترتيبها حسب الأهمية التقنية (SSBs → AI → مواد جديدة)  
- توضيح الفائدة التقنية لكل نقطة مع ذكر شركات/مشاريع فعلية  
- إضافة تحذير صغير بأن المجال متغيّر سريعاً، وهو أمر واقعي

نقاط يمكن تحسينها في الوكيل نفسه (وليس في النموذج):

1. جلب مصادر URL وليس فقط «snippets»، ثم تزويد النموذج بالنص الكامل للصفحات (سيحسن الدقة والاقتباسات).  
2. تمرير تاريخ النشر مع كل مقطع ليتجنب معلومات قديمة.  
3. طرح استعلامات فرعية باللغة الإنجليزية أيضاً ( lithium battery 2025 breakthrough site:energy.gov ) لأن أغلب المحتوى التقني موجود بالإنجليزية فقط.  
4. إضافة خطوة «إعادة كتابة الإجابة باللغة العربية» منفصلة إذا أردت نصاً عربياً خالياً من تركيبات أجنبية.

خلاصة: أداء النموذج جيد جداً بالنظر إلى أن سياقه كان مجرد مقتطفات قصيرة؛ الوكيل يعمل كالمتوقع والآن يمكنك توسيعه لجلب صفحات كاملة أو تلخيصها بالعربية مباشرة.
"""

