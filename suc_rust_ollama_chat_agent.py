# -*- coding: utf-8 -*-
"""suc_rust_ollama_chat_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SsbBNk6mr2sZXWUoMKhjXzLnwg1AhL0c
"""







curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &
!ollama pull llama3.2

"""https://github.com/Juan-LukeKlopper/OxideAgent

# Task
Install and run Rust in Google Colab.

## Install rust

### Subtask:
Install the Rust toolchain using rustup.

**Reasoning**:
Download and execute the rustup installation script.
"""

!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y

"""**Reasoning**:
Verify the installation by checking the rustc version and add rust to the PATH.


"""

!source "$HOME/.cargo/env" && rustc --version

"""## Verify installation

### Subtask:
Check the Rust version to ensure the installation was successful.

**Reasoning**:
Source the Cargo environment file and check the Rust compiler version.
"""

!source "$HOME/.cargo/env" && rustc --version

"""## Write rust code

### Subtask:
Create a simple Rust program.

**Reasoning**:
Create a file named main.rs and write a simple Rust program that prints "Hello, Rust!".
"""

with open("main.rs", "w") as f:
  f.write('fn main() {\n    println!("Hello, Rust!");\n}')

"""## Compile and run rust code

### Subtask:
Compile and run the Rust program.

**Reasoning**:
Compile and run the Rust program using the Rust compiler and execute the compiled binary.
"""

!source "$HOME/.cargo/env" && rustc main.rs && ./main

"""## Summary:

### Data Analysis Key Findings

*   The Rust toolchain, including `rustc` and `cargo`, was successfully installed in the Google Colab environment using `rustup`.
*   The installed Rust version is 1.89.0.
*   A simple Rust program named `main.rs` was created.
*   The Rust program was successfully compiled and executed, producing the output "Hello, Rust!".

### Insights or Next Steps

*   The successful installation and execution of a basic Rust program demonstrate the feasibility of using Rust within Google Colab for various tasks.
*   Further steps could involve exploring the use of Cargo for managing Rust projects and dependencies, or integrating Rust code with Python for performance-critical operations.

"""









!ollama list















"""############################################

### aØ´ØºØ§Ù„
"""

!ollama list

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!source "$HOME/.cargo/env" && cargo new rust_llama_chat

# Commented out IPython magic to ensure Python compatibility.
# %cd rust_llama_chat

!source "$HOME/.cargo/env" && cargo add reqwest --features json
!source "$HOME/.cargo/env" && cargo add tokio --features full

!source "$HOME/.cargo/env" && cargo run

!source "$HOME/.cargo/env" && cargo clean
!source "$HOME/.cargo/env" && cargo build

/content/rust_llama_chat/Cargo.toml
[package]
name = "rust_llama_chat"
version = "0.1.0"
edition = "2024"

[dependencies]
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }

/content/rust_llama_chat/src/main.rs

use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Serialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Deserialize)]
struct OllamaResponse {
    response: String,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let client = Client::new();
    let model = "llama3.2:latest";
    let url = "http://localhost:11434/api/generate";

    println!("ğŸ¦€ Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ø´Ø§Øª Rust Ù…Ø¹ Llama 3.2!");
    loop {
        print!("Ø£Ù†Øª: ");
        io::stdout().flush()?;
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;

        let input = input.trim();
        if input == "exit" {
            println!("ğŸ‘‹ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø´Ø§Øª.");
            break;
        }

        let request_body = OllamaRequest {
            model: model.to_string(),
            prompt: input.to_string(),
            stream: false,
        };

        let response = client
            .post(url)
            .json(&request_body)
            .send()
            .await?
            .json::<OllamaResponse>()
            .await?;

        println!("ğŸ¤– Llama: {}", response.response);
    }

    Ok(())
}

!source "$HOME/.cargo/env" && cargo run

"""### Ø´ØºØ§Ù„

##################################
"""







"""### ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ù†Ø§Ø¡ ÙˆÙƒÙŠÙ„ Ø¨Ø­Ø« Ø¹Ù…ÙŠÙ‚ (Deep Web Search Agent) Ø¨Ù„ØºØ© Rust ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¢ØªÙŠ:"""



/content/rust_llama_chat/src/main.rs

use reqwest::Client;
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use std::error::Error;

#[derive(Serialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Deserialize)]
struct OllamaResponse {
    response: String,
}

async fn search_web(query: &str) -> Result<Vec<String>, Box<dyn Error>> {
    let client = Client::new();
    let search_url = format!("https://html.duckduckgo.com/html/?q={}", urlencoding::encode(query));
    let res = client.get(&search_url).send().await?.text().await?;
    let document = Html::parse_document(&res);
    let selector = Selector::parse(".result__snippet").unwrap();

    let snippets: Vec<String> = document
        .select(&selector)
        .take(3)
        .map(|el| el.text().collect::<String>())
        .collect();

    Ok(snippets)
}

async fn summarize_with_ollama(context: &str, question: &str) -> Result<String, Box<dyn Error>> {
    let client = Client::new();
    let prompt = format!("Based on the following search results:\n{}\nAnswer this question: {}", context, question);

    let request_body = OllamaRequest {
        model: "llama3.2:latest".to_string(),
        prompt,
        stream: false,
    };

    let res = client
        .post("http://localhost:11434/api/generate")
        .json(&request_body)
        .send()
        .await?
        .json::<OllamaResponse>()
        .await?;

    Ok(res.response)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    println!("ğŸ” Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ù„Ù„Ø¨Ø­Ø«:");
    let mut input = String::new();
    std::io::stdin().read_line(&mut input)?;
    let question = input.trim();

    println!("â³ ÙŠØªÙ… Ø§Ù„Ø¨Ø­Ø«...");
    let snippets = search_web(question).await?;
    let context = snippets.join("\n");

    println!("â³ ÙŠØªÙ… ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬...");
    let answer = summarize_with_ollama(&context, question).await?;

    println!("ğŸ¤– Ø§Ù„Ù†ØªÙŠØ¬Ø©:\n{}", answer);
    Ok(())
}

/content/rust_llama_chat/Cargo.toml


[package]
name = "rust_llama_chat"
version = "0.1.0"
edition = "2024"

[dependencies]
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
scraper = "0.17"
url = "2.5"
urlencoding = "2.1.3"

!source "$HOME/.cargo/env" && cargo clean
!source "$HOME/.cargo/env" && cargo build

!source "$HOME/.cargo/env" && cargo add urlencoding

!source "$HOME/.cargo/env" && cargo run

"""https://www.kimi.com/chat/d2v2un13om1ns60dqo8g

Ù…Ù…ØªØ§Ø²! Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ·Ø§Ø¹:

- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø«Ù„Ø§Ø« Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ Ø¬Ù…Ø¹Ù‡Ø§ Ø§Ù„ÙˆÙƒÙŠÙ„  
- ØªØ±ØªÙŠØ¨Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© (SSBs â†’ AI â†’ Ù…ÙˆØ§Ø¯ Ø¬Ø¯ÙŠØ¯Ø©)  
- ØªÙˆØ¶ÙŠØ­ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© Ù…Ø¹ Ø°ÙƒØ± Ø´Ø±ÙƒØ§Øª/Ù…Ø´Ø§Ø±ÙŠØ¹ ÙØ¹Ù„ÙŠØ©  
- Ø¥Ø¶Ø§ÙØ© ØªØ­Ø°ÙŠØ± ØµØºÙŠØ± Ø¨Ø£Ù† Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…ØªØºÙŠÙ‘Ø± Ø³Ø±ÙŠØ¹Ø§Ù‹ØŒ ÙˆÙ‡Ùˆ Ø£Ù…Ø± ÙˆØ§Ù‚Ø¹ÙŠ

Ù†Ù‚Ø§Ø· ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„ÙˆÙƒÙŠÙ„ Ù†ÙØ³Ù‡ (ÙˆÙ„ÙŠØ³ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬):

1. Ø¬Ù„Ø¨ Ù…ØµØ§Ø¯Ø± URL ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· Â«snippetsÂ»ØŒ Ø«Ù… ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ØµÙØ­Ø§Øª (Ø³ÙŠØ­Ø³Ù† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª).  
2. ØªÙ…Ø±ÙŠØ± ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø± Ù…Ø¹ ÙƒÙ„ Ù…Ù‚Ø·Ø¹ Ù„ÙŠØªØ¬Ù†Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ø¯ÙŠÙ…Ø©.  
3. Ø·Ø±Ø­ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙØ±Ø¹ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£ÙŠØ¶Ø§Ù‹ ( lithium battery 2025 breakthrough site:energy.gov ) Ù„Ø£Ù† Ø£ØºÙ„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªÙ‚Ù†ÙŠ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙ‚Ø·.  
4. Ø¥Ø¶Ø§ÙØ© Ø®Ø·ÙˆØ© Â«Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©Â» Ù…Ù†ÙØµÙ„Ø© Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ù†ØµØ§Ù‹ Ø¹Ø±Ø¨ÙŠØ§Ù‹ Ø®Ø§Ù„ÙŠØ§Ù‹ Ù…Ù† ØªØ±ÙƒÙŠØ¨Ø§Øª Ø£Ø¬Ù†Ø¨ÙŠØ©.

Ø®Ù„Ø§ØµØ©: Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø£Ù† Ø³ÙŠØ§Ù‚Ù‡ ÙƒØ§Ù† Ù…Ø¬Ø±Ø¯ Ù…Ù‚ØªØ·ÙØ§Øª Ù‚ØµÙŠØ±Ø©Ø› Ø§Ù„ÙˆÙƒÙŠÙ„ ÙŠØ¹Ù…Ù„ ÙƒØ§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙˆØ§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ³ÙŠØ¹Ù‡ Ù„Ø¬Ù„Ø¨ ØµÙØ­Ø§Øª ÙƒØ§Ù…Ù„Ø© Ø£Ùˆ ØªÙ„Ø®ÙŠØµÙ‡Ø§ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø©.
"""

