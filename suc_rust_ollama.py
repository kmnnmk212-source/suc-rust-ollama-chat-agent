# -*- coding: utf-8 -*-
"""suc_rust_ollama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SsbBNk6mr2sZXWUoMKhjXzLnwg1AhL0c
"""







curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &
!ollama pull llama3.2

"""https://github.com/Juan-LukeKlopper/OxideAgent

# Task
Install and run Rust in Google Colab.

## Install rust

### Subtask:
Install the Rust toolchain using rustup.

**Reasoning**:
Download and execute the rustup installation script.
"""

!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y

"""**Reasoning**:
Verify the installation by checking the rustc version and add rust to the PATH.


"""

!source "$HOME/.cargo/env" && rustc --version

"""## Verify installation

### Subtask:
Check the Rust version to ensure the installation was successful.

**Reasoning**:
Source the Cargo environment file and check the Rust compiler version.
"""

!source "$HOME/.cargo/env" && rustc --version

"""## Write rust code

### Subtask:
Create a simple Rust program.

**Reasoning**:
Create a file named main.rs and write a simple Rust program that prints "Hello, Rust!".
"""

with open("main.rs", "w") as f:
  f.write('fn main() {\n    println!("Hello, Rust!");\n}')

"""## Compile and run rust code

### Subtask:
Compile and run the Rust program.

**Reasoning**:
Compile and run the Rust program using the Rust compiler and execute the compiled binary.
"""

!source "$HOME/.cargo/env" && rustc main.rs && ./main

"""## Summary:

### Data Analysis Key Findings

*   The Rust toolchain, including `rustc` and `cargo`, was successfully installed in the Google Colab environment using `rustup`.
*   The installed Rust version is 1.89.0.
*   A simple Rust program named `main.rs` was created.
*   The Rust program was successfully compiled and executed, producing the output "Hello, Rust!".

### Insights or Next Steps

*   The successful installation and execution of a basic Rust program demonstrate the feasibility of using Rust within Google Colab for various tasks.
*   Further steps could involve exploring the use of Cargo for managing Rust projects and dependencies, or integrating Rust code with Python for performance-critical operations.

"""









!ollama list















"""############################################

### aØ´ØºØ§Ù„
"""

!ollama list

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!source "$HOME/.cargo/env" && cargo new rust_llama_chat

# Commented out IPython magic to ensure Python compatibility.
# %cd rust_llama_chat

!source "$HOME/.cargo/env" && cargo add reqwest --features json
!source "$HOME/.cargo/env" && cargo add tokio --features full

!source "$HOME/.cargo/env" && cargo run

!source "$HOME/.cargo/env" && cargo clean
!source "$HOME/.cargo/env" && cargo build

/content/rust_llama_chat/Cargo.toml
[package]
name = "rust_llama_chat"
version = "0.1.0"
edition = "2024"

[dependencies]
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }

/content/rust_llama_chat/src/main.rs

use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Serialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Deserialize)]
struct OllamaResponse {
    response: String,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let client = Client::new();
    let model = "llama3.2:latest";
    let url = "http://localhost:11434/api/generate";

    println!("ğŸ¦€ Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ø´Ø§Øª Rust Ù…Ø¹ Llama 3.2!");
    loop {
        print!("Ø£Ù†Øª: ");
        io::stdout().flush()?;
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;

        let input = input.trim();
        if input == "exit" {
            println!("ğŸ‘‹ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø´Ø§Øª.");
            break;
        }

        let request_body = OllamaRequest {
            model: model.to_string(),
            prompt: input.to_string(),
            stream: false,
        };

        let response = client
            .post(url)
            .json(&request_body)
            .send()
            .await?
            .json::<OllamaResponse>()
            .await?;

        println!("ğŸ¤– Llama: {}", response.response);
    }

    Ok(())
}

!source "$HOME/.cargo/env" && cargo run

"""### Ø´ØºØ§Ù„

##################################
"""